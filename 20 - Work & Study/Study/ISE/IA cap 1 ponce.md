---
aliases:
  - Introduction to AI
booknote-plugin: true
booknote-books: 
tags:
  - artificial-intelligence
---
# Notes Overview
```dataview
list from #artificial-intelligence 
SORT file.path ASCENDING
```
Introducción Este texto recoge mis experiencias en el dictado de la asignatura desde el ciclo lectivo 2011, la permanente actualización de los conocimientos a través de distintas publicaciones especializa- das y diferentes sitios web que tratan diferentes temáticas relacionadas con la IA. También el invalorable aporte de los cursos de actualización y la lectura de papers de investigación y publi- caciones especializadas, las cuales tratan temas que no han llegado a los libros. Con esto último quiero destacar la alta dinámica en la evolución de los conocimientos de nuestra disciplina, la cual nos impulsa a permanecer casi siempre a la cola de los acontecimientos. No es posible de otra manera ya que la avalancha de novedades en IA es imparable. Debido a esto, es que en este libro introductorio nos remitiremos a los fundamentos de la IA porque han llegado a ganarse el honor de la permanencia, dejando al lector en el camino a seguir explorando y descubriendo las novedades y cambios que se van dando continuamente. Es decir que, trataremos los temas primordiales para que podamos ingresar al ecosistema de la Inteligencia Artificial con ciertas ayu- das, pero luego deberemos caminar por nuestras propias fuerzas intelectuales para encontrar la senda que nos lleve a los temas más específicos en los que desearemos ahondar. Es casi obvio decir que el panorama de la IA es muy amplio y difícil de abarcarlo completamente, pero hare- mos el intento de dar el panorama general de esta apasionante disciplina.
Capítulo 1: Los Fundamentos Históricos de la Inteligencia Artificial. Análisis Ex- ploratorio Introducción La Inteligencia Artificial (IA) ha transformado profundamente la manera en que vivimos, traba- jamos y entendemos el mundo. Desde los asistentes virtuales hasta vehículos de conducción autónoma, la IA se ha convertido en una tecnología central en la sociedad actual. Sin embargo, para comprender plenamente esta disciplina y su impacto en el presente y el futuro, es funda- mental explorar sus raíces históricas, las mentes brillantes que la desarrollaron y los hitos que marcaron su evolución. Este capítulo explora el desarrollo de la IA desde sus humildes comienzos filosóficos hasta su consolidación como disciplina científica en el siglo XX y aplicaciones tecnológicas en el siglo XXI, destacando los avances técnicos y matemáticos que la hicieron posible. Además, se analiza el impacto de los personajes clave que moldearon esta área, así como los avances recientes que han llevado a la IA a convertirse en una tecnología revolucionaria. Finalmente, se refle- xiona sobre el futuro de la IA y las oportunidades y desafíos que enfrentará en las próximas dé- cadas. 1. Primeros pasos: Los inicios del pensamiento sobre máquinas inteligentes El concepto de máquinas inteligentes no es algo exclusivo de la era moderna. La idea de repli- car la inteligencia humana en sistemas artificiales ha estado presente en la imaginación hu- mana durante milenios. Desde los mitos de la antigua Grecia hasta los autómatas mecánicos del Renacimiento, la humanidad ha soñado con crear entidades capaces de razonar, aprender y actuar de manera autónoma. Filosofía antigua y medieval El pensamiento lógico que sustenta la IA tiene sus raíces en la filosofía de la antigua Grecia. Aristóteles (384-322 a.C.), por ejemplo, desarrolló el sistema de lógica formal que más tarde se convertiría en la base de la programación computacional. Su obra sobre los silogismos pro- porcionó un marco para razonar deductivamente, un componente esencial en los algoritmos utilizados en la IA. En la Edad Media, filósofos como Al-Juarismi, conocido por sus contribuciones a las matemáti- cas y la lógica, desarrollaron métodos sistemáticos para resolver problemas, sentando las ba- ses para lo que eventualmente serían los algoritmos modernos. El Renacimiento y los autómatas mecánicos Durante el Renacimiento, distintos inventores comenzaron a construir máquinas que imitaban aspectos de la vida humana y animal. Leonardo da Vinci diseñó autómatas mecánicos, como un caballero mecánico capaz de moverse de manera autónoma. Aunque estas máquinas eran rudimentarias y carecían de "inteligencia", demostraron la posibilidad de construir sistemas que simularan el comportamiento humano. René Descartes (1596-1650), filósofo y matemático, propuso que el cuerpo humano funcio- naba como una máquina, una idea que influyó en la concepción de los sistemas artificiales como extensiones de los procesos humanos. 2. El surgimiento de la lógica formal y la base matemática.
El desarrollo de la lógica formal y la matemática en los siglos XIX y XX proporcionó las herramientas necesarias para que la IA se convirtiera en una disciplina científica.
George Boole1 y el álgebra booleana En el siglo XIX, George Boole introdujo el álgebra booleana, que permitió representar proble- mas lógicos utilizando ecuaciones matemáticas. Esta innovación fue fundamental para la crea- ción de circuitos lógicos en las computadoras modernas, ya que permitió que las máquinas procesaran decisiones binarias. Alan Turing2 y la máquina universal.
En 1936, Alan Turing publicó su artículo seminal "On Computable Numbers", en el que descri- bió la máquina de Turing, un modelo computacional teórico que podía realizar cualquier cálculo matemático si se representaba como un algoritmo. Este concepto sentó las bases para el diseño de las computadoras modernas y planteó preguntas fundamentales sobre la posibili- dad de replicar la inteligencia humana en máquinas. Luego, en 1950, Turing abordó directamente la cuestión de si las máquinas podían pensar en su artículo "Computing Machinery and Intelligence"3. En este trabajo, propuso el famoso Test de Turing, un criterio para determinar si una máquina podía exhibir un comportamiento indis- tinguible del de un ser humano. Claude Shannon4 y la teoría de la información En la década de 1940, Claude Shannon desarrolló la Teoría de la Información, que permitió comprender cómo representar y transmitir información de manera eficiente. Shannon demos- tró que los datos podían ser codificados en forma binaria, allanando el camino para el desarro- llo de sistemas digitales y lenguajes de programación. John von Neumann5 y la arquitectura de las computadoras John von Neumann introdujo el concepto de la arquitectura de programa almacenado, que permite a las computadoras almacenar tanto datos como instrucciones en una memoria cen- tral. Este diseño es la base de las computadoras modernas y facilitó la implementación de algo- ritmos complejos que son fundamentales para la IA.
3.Nacimiento formal de la IA como disciplina científica El nacimiento de la IA como disciplina científica ocurrió en la Conferencia de Dartmouth Co- llege en 1956. Este evento, organizado por John McCarthy6, Marvin Minsky7, Nathaniel Roches- ter8 y Claude Shannon, marcó el inicio formal de la investigación en IA. Durante esta conferen- cia, McCarthy acuñó el término Inteligencia Artificial y se estableció una agenda para desarro- llar máquinas que pudieran razonar como humanos. Primeros logros En las décadas siguientes, los investigadores lograron avances significativos, como:  General Problem Solver (GPS): Un programa desarrollado por Allen Newell y Herbert Simon para resolver problemas generales.  ELIZA: Un programa creado por Joseph Weizenbaum que simulaba una conversación humana básica, marcando el inicio del procesamiento de lenguaje natural.  Sistemas de ajedrez: Los primeros programas que podían jugar ajedrez, sentando las bases para el desarrollo de sistemas de IA especializados. 4. Avances desde los años 70 hasta los 90 Aunque los primeros avances generaron entusiasmo, la IA enfrentó períodos de estancamiento conocidos como "inviernos de la IA", debido a escasos resultados derivados de las limitaciones tecnológicas y la falta de financiamiento. Sin embargo, en las décadas de 1980 y 1990, los sis- temas expertos, que utilizaban reglas para resolver problemas en dominios específicos, de- mostraron el potencial práctico de la IA en la industria. 5. La explosión de la IA en el siglo XXI Con el aumento de la potencia computacional, la disponibilidad de grandes cantidades de da- tos (Big Data) y los avances en algoritmos de aprendizaje automático, la IA experimentó un renacimiento en el siglo XXI. Las redes neuronales profundas, desarrolladas por investigadores como Geoffrey Hinton9, Yann LeCun10 y Yoshua Bengio11, revolucionaron campos como la vi- sión por computadora (CV), el procesamiento de lenguaje natural (NLP) y los asistentes virtua- les (chatbots). 6. El futuro de la IA El futuro de la IA plantea preguntas emocionantes y desafiantes. ¿Llegará la IA general a igua- lar o superar la inteligencia humana? ¿Cómo abordaremos las cuestiones éticas y sociales que surgen con el uso de esta tecnología? Estas preguntas serán exploradas en profundidad en los capítulos posteriores. *Notas para referencias* - Turing, A. M. (1950). *Computing Machinery and Intelligence*. Mind. - McCarthy, J., et al. (1955). *A Proposal for the Dartmouth Summer Research Project on Artifi- cial Intelligence*. - Russell, S., & Norvig, P. (2020). *Artificial Intelligence: A Modern Approach*. 7. Las redes neuronales y el renacimiento de la IA Uno de los avances más significativos en el desarrollo de la Inteligencia Artificial ha sido el es- tudio y la implementación de redes neuronales. Este enfoque, inspirado en la estructura y el funcionamiento del cerebro humano, ha permitido a los científicos modelar sistemas que aprenden de manera similar a como lo hacen los humanos, identificando patrones y realizando predicciones.
Los inicios de las redes neuronales El concepto de redes neuronales fue introducido por Warren McCulloch12 y Walter Pitts13 en 1943, quienes propusieron un modelo matemático para describir el comportamiento de las neuronas biológicas. Este modelo sentó las bases para los desarrollos posteriores, como el tra- bajo de S.C. Kleene14 en 1956, quien exploró la representación de eventos en redes neuronales y autómatas finitos. Este enfoque permitió relacionar las redes neuronales con la computación formal. En la década de 1960, Frank Rosenblatt15 desarrolló el Perceptrón, una forma básica de red neuronal que podía clasificar datos basados en patrones simples. Aunque este enfoque mostró promesa inicial, las limitaciones matemáticas demostradas por Marvin Minsky y Seymour Pa- pert16 en su obra "Perceptrons: An Introduction to Computational Geometry" (1969), además de las limitaciones tecnológicas, llevaron a un estancamiento en la investigación de redes neuronales durante varios años.
El auge del aprendizaje profundo El renacimiento de las redes neuronales llegó con la introducción del algoritmo de backpropa- gation por Paul Werbos17 en 1975, el cual permitió entrenar redes neuronales más profundas y complejas. Este avance, junto con el aumento en la capacidad computacional y la disponibili- dad de grandes volúmenes de datos (Big Data), ha llevado al desarrollo del aprendizaje pro- fundo (Deep Learning). Este campo ha revolucionado áreas como la visión por computadora (CV), el procesamiento de lenguaje natural (NLP), y la biología computacional.
Inteligencia Artificial Capítulo 1: Historia de la IA. Análisis Exploratorio y aplicaciones Page | 10 Figura 5. Por ejemplo, Qian y Sejnowski (1988) utilizaron modelos de redes neuronales para predecir la estructura secundaria de proteínas globulares, marcando un hito en la aplicación de la IA en la biología. Más recientemente, proyectos como AlphaFold (2018, 2020) han demostrado la ca- pacidad de las redes neuronales profundas para resolver problemas científicos complejos, como la predicción de estructuras de proteínas con una precisión sin precedentes.
8.Tipos de aprendizaje en Inteligencia Artificial La IA puede clasificarse en función de los métodos de aprendizaje que utiliza. Estos métodos determinan cómo los algoritmos procesan y entienden los datos. Los principales tipos de aprendizaje son los siguientes:
Aprendizaje supervisado
El aprendizaje supervisado implica entrenar a un modelo con datos de entrada y salida conoci- das. Los algoritmos aprenden a mapear entradas específicas a salidas correspondientes me- diante la identificación de patrones. Este tipo de aprendizaje tiene aplicaciones prácticas como:  Clasificación: Por ejemplo, identificar si una imagen contiene un gato o un perro.  Regresión: Predicción de valores numéricos, como el precio de una vivienda basado en sus características. Ejemplos populares incluyen la clasificación de imágenes, la detección de objetos, y la seg- mentación de imágenes. Herramientas como Mediapipe y OpenPose han sido fundamentales para el desarrollo de aplicaciones en visión por computadora.
Aprendizaje no supervisado En el aprendizaje no supervisado, el modelo explora datos sin etiquetas y busca patrones ocul- tos o relaciones entre las variables. Este tipo de aprendizaje es especialmente útil en:
Clustering: Agrupación de datos similares, como la segmentación de clientes. Sistemas de recomendación: Sugerencias personalizadas basadas en el comportamiento del usuario, como las que utilizan Netflix o Google Photos.
Aprendizaje por refuerzo El aprendizaje por refuerzo se basa en un agente que interactúa con un entorno y aprende a maximizar una recompensa a través de prueba y error. Este enfoque ha sido clave en logros como:  AlphaGo: El sistema que venció a campeones humanos en el juego de Go.  Open AI Five: Agentes que compiten en videojuegos complejos.  DeepMind: Robots que aprenden a caminar.
9.Desafíos de Seguridad Veamos algunos casos ejemplos que nos darán una visión abarcante del tema seguridad: Primer caso: Zillow. La empresa inmobiliaria Zillow tenía un modelo de negocios secundario en el que, al detectar que si una propiedad puesta a la venta estaba por debajo del valor de mercado y/o que podría adquirir un mayor valor de reventa en el futuro, Zillow no publicaba el anuncio de venta de la propiedad sino que, directamente, la compraba al precio menor con el objetivo de revenderla a un precio mayor a algún cliente futuro. La decisión de compra de la propiedad estaba respaldada por los datos de los tasadores huma- nos y por un algoritmo de IA de tasación entrenado para predecir el posible valor futuro.
En determinado momento, este esquema de negocios falló y la empresa perdió 300 millones de dólares en pocos meses. Entonces, ¿qué pasó con la IA? Este ejemplo es interesante para hablar de Aprendizaje Supervisado, pues el algoritmo de tasación fue entrenado bajo ciertas condiciones económicas puntuales, tales como una sobrevaluación de los bienes raíces, ciertas tasas de interés bancario, cierto nivel de actividad de la economía, determinado índice de la construcción, cierta tasa de inflación anual, etc., etc. Es decir, una cantidad de condiciones muy particulares del entorno económico-financiero del país. Para esas condiciones específicas, el algoritmo de tasación con IA funcionaba perfectamente y así lo hizo durante varios años. El problema surge cuando se produjo un cambio en las condiciones económicas. Un ejemplo po- dría ser una variación inesperada en la tasa de inflación, algo que no puede anticiparse. Otro ejemplo, los cambios en las tasas de interés fijados por la Reserva Federal (FED) que no esta- ban dentro las características del problema (features) o inputs del dataset con el que fue en- trenado. Esto es así porque es imposible saber con anticipación cuál será la tasa de interés dentro de, por ejemplo, siete meses. Existen muchas características que pueden variar en el tiempo que no pueden ser anticipados. Por lo tanto, los pronósticos de la IA eran válidos sólo bajo esas ciertas condiciones y ya no eran valederos si dichas condiciones sufrían alteraciones más allá de ciertos límites muy acotados. Este ejemplo nos sirve para comprender el hecho de que cuando entrenamos una IA con un cierto dataset y la IA aprendió y predice correctamente, dichas predicciones son válidas dentro de los límites que le hemos establecido con nuestro entrenamiento y testeo. Si las condiciones difieren de las utilizadas en el entrenamiento, entonces comenzará a entregar resultados no válidos, equívocos.
Segundo caso: Apple Card En este ejemplo que veremos, se resalta la importancia del set de datos a utilizar.
Si no disponemos de un dataset que refleje la realidad del ambiente en el que va a trabajar la aplicación, entonces más temprano que tarde, nos encontraremos con problemas que, a ve- ces, son de fácil resolución y otras no tanto. Estos sesgos de los datasets pueden generar pro- blemas en la etapa de explotación de la aplicación, tal como ocurrió en Apple Card. Agencias federales de Estados Unidos investigaron Apple Card tras denuncias por discrimina- ción de género. Un destacado desarrollador de software dijo en Twitter que la tarjeta de cré- dito era sexista contra las mujeres que solicitaban crédito. Esto fue muy mala publicidad para la imagen pública de la empresa y repercutió en su valoración y prestigio. El escándalo fue gra- vísimo por las reivindicaciones feministas en boga en esa época. El problema obligó a que Apple Card retirara de servicio la IA, con las lógicas consecuencias de aumento de costos administrativos, y que los desarrolladores se pusieran a reentrenar la IA. Luego de un mes de trabajo se pudo poner nuevamente en funcionamiento la aplicación.
Las entidades reguladoras estatales investigaron el algoritmo de Apple Card, que se utilizaba para determinar la solvencia crediticia de los solicitantes. Este caso surge porque a las mujeres solicitantes del servicio se le asignaba una cantidad de crédito inferior a la de los solicitantes hombres. ¿Por qué razón ocurrió esto? Luego de investigar el problema y comprobar que todo funcionaba bien y de acuerdo con las especificaciones de la empresa y las regulaciones estatales, los investigadores se dieron cuenta que el dataset empleado en el entrenamiento de la IA, en realidad, tenía muy pocos casos de mujeres lo cual producía un sesgo importante que se veía reflejado en los montos de crédito asignados y de este modo, podemos decir que la aplicación era “machista” en sus predicciones de asignación de los montos de crédito a las solicitantes.
Tercer caso: cámaras inteligentes de Amazon. La empresa Amazon instaló cámaras que filmaban cómo conducían los choferes de las camio- netas y camiones de reparto de sus mercaderías vendidas desde la aplicación. Estas filmacio- nes eran supervisadas por una IA que calificaba la manera de conducir de los choferes para premiarlos o para castigarlos en caso de que cometieran faltas en su trabajo. El objetivo de Amazon era minimizar los cargos generados por las multas de tránsito de sus vehículos de re- parto y dar una imagen de empresa que respetaba las legislaciones y ordenanzas viales. El problema comienza cuando una gran cantidad de estos conductores son penalizados por la IA aduciendo que habían cometido errores en su trabajo. El sistema no incluía ninguna revisión humana y la penalización se hacía en forma totalmente automática. Entonces, los conductores protestan porque la IA les asignaba faltas y errores que ellos no habían hecho. ¿Qué pasó con la IA? Las cámaras instaladas a bordo de los vehículos grababan en forma continua el manejo de los conductores quienes los hacían bajo condiciones cambiantes de tránsito y de clima. Así, los reflejos de la nieve en los caminos, los destellos de los relámpagos en los días de tormenta y otras condiciones normales que se daban durante la conducción, deslumbraban a las cáma- ras y cegaban parcialmente la visión completa de las escenas de conducción. Estas situaciones (lluvias, nevadas, alumbrado público en posiciones particulares, rayos, etc.) determinaban las respuestas de la IA por la sencilla razón de que estas no eran las condiciones con las que fue entrenada. Nuevamente, tenemos otro ejemplo de lo importante que es la obtención de data- sets adecuados para el entrenamiento de las IA’s.
Cuarto caso: la IA de los autos Tesla confunde la luna llena del anochecer con un semáforo en luz amarilla. Una situación particular de los vehículos de conducción autónoma fabricados por la empresa Tesla se dio cuando algunos usuarios descubrieron que su auto confundía la Luna llena con un semáforo en luz amarilla (precaución), circunstancia que normalmente se da antes del cambio de verde a rojo y viceversa. En tal situación, el auto se detenía a la espera del cambio de luces del “semáforo”, cosa que no ocurriría nunca. El vehículo permanecía detenido en el medio de la calle, algo poco conveniente en una calle con mucho tránsito.
Lógicamente, esta situación acaecía sólo cuando el vehículo se trasladaba en conducción autó- noma, la dirección de la calle coincidía con la posición de la Luna en un lugar específico del campo de visión del auto que podría considerarse la posición de un semáforo, con condiciones meteorológicas de niebla, alta humedad, aire sucio o contaminado. Al ser una circunstancia muy particular en la que tenían que concurrir una serie bastante parti- cular de eventos, nunca fue tomado en consideración por el equipo que confeccionó el dataset de entrenamiento. En ese escenario, la IA daba su mejor respuesta, si bien ésta no era la ade- cuada para la situación.
Quinto caso: Con motivo de la pandemia del virus del COVID, se desarrollaron varias IA para ayudar en el diagnóstico de esta enfermedad. Veamos ahora el caso de una IA entrenada con radiografías de tórax de casos positivos de COVID. Una vez entrenada y testeada, esa IA fue puesta en pro- ducción para ayudar al personal médico y se aseguraba que la detección (verdaderos positivos) llegaba a un 99%. Todo marchaba bien, hasta que se notó una particularidad muy relacionada con las características propias del COVID que invalidó todo el trabajo de esta IA.
En la entidad médica donde se desarrolló la IA se contaba con dos dispositivos radiográficos para tomar las placas de tórax, pongamos por caso el dispositivo A y el dispositivo B. Estos dis- positivos producían imágenes con una casi imperceptible diferencia, que no era advertida nor- malmente por los profesionales radiológicos porque las imágenes de ambos dispositivos ser- vían perfectamente para los diagnósticos médicos. Cuando se produce el brote pandémico, y atendiendo a la contagiosidad que se creía tenía el COVID, los médicos deciden destinar uno de los dispositivos de radiología exclusivamente para aquellos pacientes que tenían alta chance de estar enfermos de COVID, supongamos, el A. Esta sospecha de COVID positivo se determinaba mediante exámenes previos de los signos y sínto- mas observados en la examinación clínica. Esto es, si había una fuerte sospecha de contagio, se enviaba el paciente a realizar la radiografía de tórax que confirmaría el diagnóstico en el dispo- sitivo designado con exclusividad para estos casos, el A. Mientras tanto, a todos aquellos pa- cientes que necesitaban radiografías de tórax por distintos motivos y no presentaban indicios de contagio por COVID, se los enviaba al otro dispositivo de radiografía, es decir el B. La IA recibía imágenes de ambos equipos de radiología (A y B) sin que se le especificaran cuál era el origen de cada imagen. Las predicciones diagnósticas parecían ser tan buenas que animó a los profesionales a publicar sus trabajos en revistas de investigación médica, con la espe- ranza de que la IA fuera una ayuda significativa para otras instituciones. Al probar la IA en otros equipos radiológicos, se comenzó a evidenciar proliferación de diagnósticos con falsos positivos y falsos negativos de COVID. Esto obligó a un completo estudio de todo el proceso y se descubrió que lo que la IA en realidad había aprendido no era tanto a diagnosticar acertada- mente si un paciente estaba enfermo de COVID sino a distinguir cuál era el origen de las imá- genes que le llegaban, si del equipo A o del equipo B. Al coincidir que el total de los casos sos- pechosos de COVID pasaban por el equipo A y todos los demás pacientes por el equipo B, la IA reconocía las imágenes que provenían del equipo A y no las que venían del B. Este aprendizaje de distinguir la fuente de las imágenes fue mal interpretado por los profesionales médicos, quienes creyeron que la IA predecía (o confirmaba) con alto grado de precisión los casos de COVID positivo, cuando en realidad lo que hacía era distinguir entre los orígenes de las imáge- nes. El artículo de investigación cuya portada se muestra en la Figura 27 trata el tema de la IA co- mete esos errores de diagnóstico.
En este ejemplo, vemos la importancia de que en nuestro dataset no aparezca alguna variable “confundidora” que produzca un sesgo en los resultados de la IA. Esto se debe tener en consi- deración en el momento que hacemos nuestro análisis inicial, esto es, tener mucha precaución sobre la existencia de alguna/s variable/s que sea/n tan importante/s dentro del data set que opaque la importancia de la o las variables que son el objeto de la predicción del modelo a entrenar. Cuando se construye el dataset de entrenamiento de la IA debemos tener en cuenta el con- texto en su totalidad para impedir que la IA aprenda sobre una variable que noe la de interés del trabajo o proyecto en ejecución. Apartándonos un poco de lo anterior y como una forma de ver la evolución de una rama de la IA, tal como es la Computer Vision, en las figuras siguientes veremos un par de casos en los que para 2015, la IA se confundía. Actualmente, ya no ocurre eso, pero es bueno ponerse en el lugar de la IA y así comprender la complejidad a resolver.
Con los ejemplos anteriores en mente, podemos abordar el tema de cómo prevenir la parición de estos errores que darían lugar a sesgos en la construcción de nuestros datasets. 10. Sesgos en los datos y desafíos éticos A modo de introducción al tema, veamos algunos ejemplos clásicos. Durante la Segunda Gue- rra Mundial (1939-1945) se utilizaron aviones bombarderos en ambos bandos contendientes (Alemania nazi vs. Aliados). Su trabajo era transportar la mayor cantidad de bombas de alto explosivo hasta territorio enemigo y descargar allí todas las bombas en los blancos elegidos por los servicios de inteligencia con la finalidad de destruir las instalaciones y recursos militares (también civiles, a pesar de la prohibición de hacerlo) a fin de privar al enemigo de recursos bélicos y materiales útiles para la confrontación. Lógicamente, el enemigo en tierra disponía de mecanismos de defensa antiaérea como cañones y aviones de caza con los que atacaban a los aviones bombarderos para inhabilitarlos y que no puedan cumplir con su misión de destruc- ción a cabalidad. Tomemos por caso los bombarderos de los países aliados (Inglaterra, Estados Unidos, URSS y otros) atacando a la Alemania Nazi y sus adláteres (Italia, Japón, etc.) Obviamente, en muchos casos la defensa antiaérea de los cañones y los aviones caza eran exi- tosos en derribar los aviones bombarderos aliados atacantes, mientras que en otras ocasiones los aviones y sus tripulaciones eran lo suficientemente afortunados como para regresar a sus bases de origen. En las ocasiones que los aviones bombarderos retornaban a sus aeródromos de origen, se podía ver claramente los daños sufridos por las máquinas en sus alas y fuselaje. Pronto, los diseñadores e ingenieros aeronáuticos desarrollaron el siguiente esquema que ve- mos en la Figura 30. Aquí se muestran las zonas de mayor probabilidad de impactos de la me- tralla del fuego antiaéreo y de los disparos de los aviones caza enemigos representadas por los puntos rojos. Ante este panorama, surgió la pregunta ¿cuáles son las áreas de los aviones que debemos re- forzar para que los bombarderos puedan sufrir un gran castigo por parte del fuego enemigo y aun así regresen a salvo a sus aeródromos? Tal vez, una respuesta rápida y descuidada sería: “¡Reforcemos las áreas donde se dan la ma- yor probabilidad de impactos de la metralla del enemigo!”. Esto implicaría estudiar esas zonas donde más impactos los aviones han recibido, en promedio y de esa forma garantizar que esas partes no se verán tan afectadas por la metralla. Bueno, pensar de esa forma es un error. Pensemos que si hemos podido determinar en qué zonas del avión se dan la mayor cantidad de disparos se debe a que los aviones averiados han podido regresar a pesar de estar acribillados por la metralla de los disparos de artillería antiaé- rea y los disparos de los cañones de los aviones caza defensores. En otras palabras, los aviones que regresaron del frente de batalla lo hicieron a pesar de los grandes daños sufridos en sus alas o en su fuselaje. Esto indica que aunque hayan perdido gran parte de su estructura, aun así pudieron maniobrar y llegar a destino. En cambio, aquellos que sufrieron daños en OTRAS partes de su estructura NO regresaron, indicando que las partes de los aviones que se deben resguardar, blindar o reforzar son aquellas que no tienen puntos rojos, porque aquellos avio- nes que fueron alcanzados en esas áreas fueron derribados, por eso no pudieron regresar a sus bases aéreas. En este ejemplo se hace claro que la decisión de blindar tiene que ser el complemento del con- junto de datos de los impactos recibidos por los bombarderos. Uno de los principales desafíos en la implementación de la IA es el manejo de sesgos en los conjuntos de datos. Los sesgos pueden surgir de diversas fuentes, como:  Sesgo de muestreo: Cuando los datos no representan adecuadamente a la población objetivo. Sesgo de confirmación: Cuando los datos refuerzan creencias preexistentes.
Efecto Hawthorne: Cuando los sujetos cambian su comportamiento porque saben que están siendo observados. Sesgo de Falsa causalidad: El website tylervigen.com se ocupa de buscar y encontrar correlaciones absolutamente disocia- das para reírse de las publicaciones y aún de artículos de investigación que fuerzan el concepto estadístico de correlación para que aparezca que existe una causalidad. Pero correlación no es causalidad y es importante reconocer esto cuando se trabaja en la confección de datasets. Por ejemplo, en la Figura 34 se han tomado datos del consumo de margarina en el estado de Maine y datos de la tasa de divorcios entre los años 2000 y 2009. Asombrosamente, el índice de correlación es altísimo: ¡¡99, 26%!! Por supuesto, nada tienen que ver estos datos, pero para un observador descuidado y que no comprenda el concepto de correlación estadística, podría concluir que el consumo de margarina eleva la cantidad de divorcios en el estado de Maine. En la Figura 35 tenemos el siguiente ejemplo de correlación espuria. En el diagrama se ve la co- rrelación entre las importaciones de petróleo crudo en los Estados Unidos durante la década que va desde 2000 a 2009, y el consumo de carne de pollo en el país, siendo el valor de esta correlación de ¡89,99%! Obviamente, importar petróleo no dispara el consumo de carne de po- llo ni mucho menos. Es una correlación espuria y la lección que debemos aprender es a no de- jarnos engañar por los números y a utilizar nuestro sentido común para todo, lo cual incluye también la confección de datasets para entrenamiento de modelos de IA. Para terminar con estos ejemplos, en la Figura 36 se muestra el inicio del sitio web de Tyler Vi- gen de donde obtuvimos estos ejemplos de correlaciones espurias. El último sesgo que veremos:  La falacia de McNamara18 o falacia cuantitativa: cómo podemos ver en la nota al pie, Robert McNamara tuvo una importante carrera política y administrativa de alto nivel. De su experiencia ejecutiva cosechó una serie de observaciones, normas o consejos que podemos resumir en las siguientes cuatro: 1. Medir todo lo que puede ser fácilmente medido. 2. Descartar todo lo que no se pueda medir fácilmente. 3. Asumir que lo que no se puede medir no importa. 4. Asumir que lo que no se puede medir fácilmente no existe. ¿Por qué llamamos Falacia de McNamara a este sesgo? McNamara intentó cuantificar el éxito en la guerra de Vietnam considerando como una variable objetiva y medibles a la cantidad de bajas sufridas por el enemigo dejando de lado otras variables que no eran cuantificables, tales como el apoyo que el pueblo vietnamita daba a los guerrilleros del Vietcong, el desconoci- miento del terreno por parte de los militares estadounidenses, la oposición de público ameri- cano a la guerra, etc. De este modo redujo la guerra a un modelo matemático muy reduccio- nista, ignorando que el tipo de guerra que estaban librando (guerra de guerrillas) donde la re- sistencia estaba dispersa por toda la geografía y donde la estimación de las bajas enemigas era inevitablemente imprecisa. El autor Daniel Yankelovich publicó en 1972 un artículo donde criticaba este sesgo de la si- guiente forma: “Pero cuando la disciplina de McNamara se aplica demasiado literalmente, el primer paso es medir lo que se pueda medir fácilmente. El segundo paso es ignorar lo que no puede medirse fácilmente o tener un valor cuantitativo. El tercer paso es suponer que lo que no se puede medir fácilmente no es realmente importante. El cuarto paso es decir que lo que no se puede medir fácilmente no existe. Esto es suicidio.” Completando el panorama de los sesgos, debemos tener considerar que estos sesgos pueden llevarnos a resultados injustos o inexactos, especialmente en aplicaciones críticas como la toma de decisiones médicas o financieras. Por ejemplo, el caso de las cámaras de IA de Ama- zon que penalizaron a conductores por errores que no cometieron destaca la importancia de abordar estos desafíos éticos.
10.Validación ¿Cómo puedo garantizar que un modelo funciona? Debemos tener en cuenta que nuestros al- goritmos tienen la capacidad de memorizar los datos que le vamos entregando en el entrena- miento, es decir si el modelo memoriza un ejemplo que le dimos en el entrenamiento, enton- ces ese ejemplo no sirve para comprobar que tan bien funciona el modelo. Debido a esto es que separaremos a nuestro conjunto completo de datos en tres partes: una parte la usaremos para el entrenamiento del modelo, otra parte totalmente distinta será utili- zada por quien está desarrollando el algoritmo para la validación del modelo en una serie de varios ciclos entrenamiento-validación; y lo hará hasta quedar satisfecho con su ejecución. Por último, la última parte la usaremos para testear (o probar) y que nos dará una métrica con la que nos podremos quedar tranquilos de que no introdujimos ningún sesgo en el proceso. Esta es la métrica que podemos informar de la precisión final del modelo. Algunos dicen que basta trabajar con las dos primeras partes del data set, pero hay una buena razón para trabajar con las tres partes, y es la ventaja de quedarnos tranquilos de que, ha- biendo entrenado y validado el modelo, éste debe ser capaz de funcionar y dar sus prediccio- nes correctamente con nuevos datos que nunca ha tratado. Esto implica que esta última parte del dataset original no puede contener ningún caso que haya estado en las primeras partes utilizadas en entrenamiento y validación. Debe ser un set completamente diferente, no contaminado con registros repetidos, datos totalmente nuevos y diferentes de todos los casos o datos que estaban en las primera y segunda partes del dataset original. Ahora, para concluir el tema veamos el gráfico de la Figura 38. En el eje vertical tenemos la magnitud del error del modelo (cuanto más altos los valores, mayor es el error) mientras que en el eje horizontal está la capacidad del modelo. Nos interesa la línea roja del gráfico. Con los primeros sistemas que estudiaremos, los cuales tienen una baja capacidad (podríamos decir “menos inteligentes”), obtendremos errores más altos. Sin embargo, se puede observar que incrementando un poco la capacidad nos acercamos a la capacidad óptima, es decir cuando el error tiene un mínimo. Ahora bien, a medida que vamos incrementando la capaci- dad del modelo, vamos entrando en otro error que es el error de Varianza, que es cuando el algoritmo comienza a memorizar mucho del universo de datos de entrenamiento. Cuando tra- bajamos con algoritmos muy básicos, como los primeros que veremos a continuación, a éstos les cuesta mucho memorizar por lo que no debemos preocuparnos por el error de varianza. Por el contrario, cuando vamos a algoritmos de mayor capacidad, tales como las Redes Neuro- nales, el error no consiste en quedarnos “cortos” sino en “pasarnos” de la capacidad óptima para el problema y que, al pasarnos, el algoritmo memorice el dataset de entrenamiento y las métricas iniciales nos den excelentes resultados, pero que cuando lo llevemos a datos de la vida real o a otros set de datos, las predicciones sean completamente erróneas. Éste es el error de generalización. En el gráfico vemos cómo al pasar el límite óptimo de la capacidad, el error comienza a crecer. Veamos un ejemplo: supongamos que estoy tratando de predecir el precio de cinco viviendas y graficamos en un dispersograma precio en función del tamaño. Si utilizamos un algoritmo muy básico, es decir, de baja capacidad tendremos un error denominado “underfitting”en el que predomina el valor de sesgo (bias alto) y en el gráfico de la izquierda de la Figura 39, la línea roja de nuestro modelo pasa entre los distintos puntos que representan la relación precio-ta- maño de las casas y se aprecia claramente que no concuerda con la distribución que tienen los distintos puntos, al punto que ninguno está contenido en la recta definida por el modelo. En el gráfico del medio tenemos la traza de un algoritmo de nivel superior, es decir de mayor capacidad. En este gráfico, la línea roja es curva y se acerca mucho más a las posiciones de los pares precio-tamaño, indicando que está más cerca de la realidad y que sus predicciones serán mucho más certeras que las del modelo lineal de la izquierda. Es decir, el modelo no es per- fecto y lo comprobamos observando que los pares no caen exactamente sobre la curva roja sino que existe una cierta distancia que representa el error predictivo del modelo. Nuestra de- cisión está en determinar si toleramos ese error o si queremos ir a un modelo más preciso y, por consiguiente, más complejo. Si desarrollamos un modelo de mayor capacidad, es decir un algoritmo más complejo que po- demos ver en el gráfico de la derecha. Se aprecia que es una curva polinómica de grado supe- rior (3er grado o más), entonces la coincidencia con los pares precio-tamaño es perfecto ya que la curva pasa por todos los puntos. Ahora bien, si ahora le añadiéramos al set de datos una nueva propiedad que se represente como un par precio-tamaño que quede fuera de la curva, entonces el modelo tendrá errores para predecir porque el modelo sólo quedará válido para esas cinco propiedades originales y nada más. Aquí vemos el concepto de que el modelo “memorizó” los datos y queda inflexible sobre ese conjunto de datos. Nos hemos pasado de la capacidad óptima y estamos en un caso de “overfitting” en donde el peso de la varianza es grande. Este ejemplo nos dice que ha medida que aumento la complejidad del algoritmo del modelo, iré pasando de un error a otro, del error dado por el alto sesgo (underfitting) a un error dado por una alta varianza (overfitting). Por supuesto, no hay que olvidar que hay un límite entre es- tos dos errores y es el de la capacidad óptima del modelo para poder predecir con un cierto margen de error minimizado respecto de las otras posibilidades. Aquí, en esta capacidad óp- tima los dos tipos de error son mínimos. Entonces, y muy importante, no debemos caer en la falacia de lograr siempre que un modelo tenga la capacidad máxima posible, porque al hacerlo también estamos induciendo error. 11. Introducción a la codificación en Google Colab. YData-Profiling Usaremos para los ejemplos de codificación el ambiente de Google Colab porque es de acceso gratuito disponiendo simplemente de una cuenta en Gmail, es decir una cuenta Google. Una gran ventaja de trabajar aquí es que ya está disponible un entorno Python para ser utili- zado inmediatamente sin tener que instalar nada en nuestra computadora. Cuando nos logueamos en Colab, establecemos una sesión individual y única en algún servidor en alguno de los muchos data centers de Google. Cada sesión dispone de todas las principales librerías de Python con las que podemos trabajar, pero si llegáramos a necesitar alguna librería especial, se puede instalar fácilmente con el comando !pip en alguna de las celdas de código. Por ejemplo, en la Figura 40 vemos la instalación de la librería ydata-profiling. Recordemos que Colab dispone de dos tipos de celda: la de +Code, es decir para ingresar có- digo y la +Text que permite escribir comentarios más explicativos que aquellos que podemos poner dentro del código utilizando el # (hashtag) como primer carácter de la línea. Para el caso que necesitemos una celda de Text, en ella dispondremos de algunos comandos para editar el texto que ingresemos. Además, acepta el ingreso de comando Látex y los inter- preta de tal modo que podremos ingresar y editar fórmulas matemáticas, físicas, etc. con cali- dad de libro. Como apreciamos en la Figura 40, Colab nos invita a comenzar a codificar o a generar código utilizando la IA Gemini ya instalada, lo cual nos facilita el desarrollo del código pues nos puede ayudar dándonos ideas de resolución de problemas o dándonos el código adecuado para lo que necesitemos hacer. A la izquierda del editor tenemos una barra donde disponemos de varias herramientas. La úl- tima es la de Files, desde la que nos permite subir (upload) archivos que usaremos en nuestro código. Los archivos que queremos utilizar en nuestra aplicación simplemente pueden ser arrastrados y soltados en el área de Files para que quede una copia disponible dentro de nues- tro espacio de trabajo. Es importante recordar que cada sesión de Colab tiene una duración máxima de 4 horas a par- tir del momento que dejemos de trabajar. Esto significa que, luego de ese período de tiempo, Colab destruye la sesión con todo su contenido, motivo por el cual si queremos conservar có- digo o archivos tenemos que ocuparnos de descargarlos en algún lugar de nuestra máquina. Bien, supongamos disponer de un archivo grande del tipo planilla de cálculo como Excel y ne- cesitamos ver cómo son los datos. Necesitamos disponer de la librería YData-Profiling que tenemos que instalar ya que no está instalada en forma predeterminada. Al ejecutar la celda, tendremos la Figura 41, la cual es una captura de pantalla de la instalación de los distintos módulos y dependencias necesarias en la instalación de la librería. Cuando apa- rece una línea que comience con “Requirement already satisfied:….” está indicando que ciertos paquetes ya están instalados en el ambiente, normalmente por otra librería. Para probar un poco de código vamos a traer un archivo en formato CSV (comma separated values) que no es otra cosa que un archivo de texto plano, donde los distintos campos de datos de un registro están separados por comas. MS Excel reconoce este tipo de archivo, por eso es por lo que se muestran con el logo de Excel. Esto ocurre sólo si Excel está instalado en el sis- tema. Para el caso ejemplo que veremos ahora en la Figura 42, el archivo que usaremos es de exten- sión md, el nombrado como variables-inmobiliarias-es.md19. Lo arrastramos hasta el área de Files para subirlo al directorio de nuestra sesión Colab y vamos a probar la IA preinstalada y le pedimos: “give me the code to load variables-inmobiliarias- es.md and print it”. Podemos apreciar que, a pesar del inglés “tarzanesco” utilizado en la petición a la IA, esta es lo suficientemente generosa como para responder sin ofenderse y nos entrega un código Python muy simple en una celda de la notebook. Aún no le hemos dado un nombre, por eso Colab la denomina temporalmente como Untitled3.ipynb. Aclaremos que le podemos pedir cosas a la IA en cualquier idioma, incluyendo nuestro amado español. La celda con el código puede ejecutarse clickeando el iconito circular con la flecha que se en- cuentra a la izquierda de la celda. Debajo de esta celda nos imprime los resultados solicitados que, como podemos apreciar, son características de viviendas en poder de una cierta empresa inmobiliaria y el archivo describe esas características y los posibles valores que pueden asumir. Podemos ver, por ejemplo, que la variable HouseStyle puede asumir distintos códigos, cuyos significados están explicados en la columna siguiente hacia la derecha. Bien, yendo al tema de la herramienta de la librería YData-Profiling, veremos que es una herra- mienta muy buena para entender los datos con los que estamos trabajando. Regresamos al ejemplo: En cuanto al código, vemos la importación de la librería Pandas y permite leer archivos de ta- blas, como los CSV, que vemos en la línea de código siguiente: df=pd.read_csv(“txs_with_headers_V3.csv”). Si ahora, en la siguiente celda de código escribo la variable df que recibió el resultado del read, me muestra los datos almacenados: Apreciamos que la salida muestra las primeras 5 filas de la tabla y también las últimas 5 filas. Además, informa el tamaño de la tabla que es de 23.090 filas por 15 columnas. Lógicamente, los campos de los atributos son las 15 columnas y las 23.090 filas son los registros del archivo. Colab también nos sugiere algunas acciones a realizar en el renglón Next steps:, por ejemplo, Generate code with df, es decir nos pregunta si queremos seguir codificando otras cosas con la variable df, también nos da la posibilidad de habilitar la genera plots recomendados según el tipo de datos que estamos trabajando, con View recommended plots o generar una nueva hoja interactiva de esos datos con New interactive sheet. Todas estas opciones responden a la in- corporación de la IA Gemini a Colab. Los datos que vemos corresponden a una lista de transacciones de un supermercado. Si com- pletamos la línea df con la sugerencia df.head(), nos mostrará los 5 primeros registros de la ta- bla. Con tail mostraría los últimos 5. Recomendamos animosamente recurrir a la documentación de toda librería que se use en cualquier código. Las librerías suelen ser MUY amplias y se están actualizando constantemente y a veces es difícil recordar todas las opciones disponibles. Muchas veces sucede que conocer todas o una gran parte de las opciones nos ahorra desarrollar código por nuestra cuenta. Aquí la página de la documentación Pandas en su versión 2.2. Posiblemente, en el momento que leas esto, ya haya otras versiones estables de Pandas. Bien, volviendo a nuestro ejemplo, una vez que tenemos el ProfileReport, podemos hacer en la siguiente celda Ahora lo pasa a una archivo de formato HTML Este informe generado nos da un resumen estadístico exhaustivo de los datos almacenados en el archivo. El informe obtenido al hacer to_notebook_iframe permite revisar variable por variable: Utilizando profile.to_file() podemos enviar todo a un archivo en formato HTML con un nombre de archivo que se alojará en Files. Esto nos permite compartirlo mediante un mail o alguna otra forma similar. Este HTML lo puedo ver en cualquier navegador y muestra todo lo referente al archivo CSV y, como podemos ver, nos da una columna de estadísticas de nuestro dataset y otra columna de- finiendo las cantidades de los tipos de variables (numéricas, de fecha, de texto y categóricas). Muy importante para la caracterización del dataset, podemos ver las Alerts (en este caso, 14) donde se analiza cada variable y nos advierte de situaciones que pueden pesar negativamente en nuestro trabajo posterior. Nos señala cuando hay desbalances (imbalanced) en los datos al- macenados en una variable, cuando hay valores perdidos (missing values), cuando hay sesgo (asimetría o skewed) en la distribución de los datos de una variable, si la variable contiene va- lores únicos en todos los registros y, por último, el porcentaje de ceros existentes en los regis- tros de una variable. Esta información es sumamente útil para nuestro trabajo a continuación. Luego viene la parte de Variables y aquí analiza variable por variable, como se aprecia en la Fi- gura 55. Para la variable ARS que discrimina en qué moneda se realizaron los pagos, tenemos En la siguiente Figura se nos muestra el resultado de clickear en el botón More details Aquí se discrimina el tipo de pago (tarjeta de crédito o Mercado Pago) en la variable categórica creditCard. Por supuesto, en esta salida de la librería YData-Profiling no estamos haciendo Machine Lear- ning, pero estamos obteniendo conocimiento valioso del dataset para luego poder adaptarlo a las tareas de entrenamiento, evaluación o prueba. Tomemos por ejemplo la Figura 56 donde están los pagos en pesos argentinos y en dólares. Al ver el gráfico de barras tenemos que dar- nos cuenta de que esta variable no nos servirá para el análisis porque la diferencia entre pesos y dólares es tan grande que ya estamos partiendo con un sesgo local que no refleja la realidad fuera de nuestro país. Veamos otro ejemplo con los valores perdidos en las variables: En este gráfico de barras tenemos las variables (aclaro que faltan algunos headers de las co- lumnas) y se nos muestra claramente que las primeras ocho desde la izquierda están igualadas en que no tienen valores perdidos, así como las columnas 10 y 11. Sin embargo las columnas 9 y las últimas 4 tienen una notable cantidad de valores perdidos. Deberemos trabajar sobre es- tas variables para determinar si son aptas para ingresarlas dentro de nuestro análisis. Esta pérdida de valores puede llevarnos a tener que descartar la columna del dataset o realizar otra operación, denominada imputación, que es reemplazar los valores perdidos por valores que serían “razonables” que estén allí. ¿Cómo logramos esto? Bien, supongamos que tengo va- lores perdidos en la columna Sueldo de nuestros clientes y supongamos que ésta es una varia- ble que nos sirve para determinar otras cosas, como por ejemplo el nivel crediticio que le po- demos otorgar a dicho cliente. Entonces, dado que en la base de datos tenemos otras colum- nas con datos de los clientes, como edad, nivel educativo, tipo de ocupación, etc. podríamos inferir que un cliente con determinadas características de nivel educativo, edad, profesión, ocupación, etc. debería tener un cierto Sueldo simplemente buscando entre los otros clientes de características similares que tenemos entre nuestros datos. Esta es una manera “razonable” de recuperar datos perdidos. Por supuesto, no funciona en todos los casos: si el cliente solici- tante de un crédito es un cantante villero de cumbia o de reggaetón, que apenas ha terminado su educación primaria, no tiene antecedentes de trabajo o de ingresos fijos, su edad es menor a los treinta años y no tiene una ocupación fija, posiblemente su nivel crediticio no sea muy bueno en principio. Pero, si estudiamos el caso y resulta que ese “cantante” es exitoso en ven- der su cuestionable producto y éste le produce ingresos considerables, entonces su nivel credi- ticio será muy alto. Casos de este tipo, hay muchos en el mundo latino de la música popular o de los deportes. Más detalles, a continuación en la solapa Categories: En la Figura el informe está dado por la categorías y podemos apreciar que, de las distintas ca- tegorías, algunas como account_money, visa y debvisa están más o menos representadas den- tro del dataset. Sin embargo todas las otras categorías tienen muy pocos registros, y esto hace que no estén bien representadas en el conjunto de las otras variables. Aquí, lo que se suele ha- cer es una recategorización, uniendo variables para lograr una cantidad que pueda ser repre- sentativa frente a otras variables y, de esta manera, evitar el sesgo. Si unimos debvisa junto al resto de las variables, podríamos obtener una columna “Otros medios de pago” que llegaría a tener un 27, 8%. De este modo la recategorización nos dejaría sólo 3 variables: account_mo- ney, visa y otros_medios_pago. Lógicamente, aún deberíamos resolver el asunto de los valores perdidos que suman un 23,6% nada despreciable, pero veremos algunas estrategias para resolver esto. Algunas soluciones ya han sido anticipadas y podremos aplicarlas según las características específicas de la variable donde se produjeron los valores perdidos. Ahora, para finalizar el ejemplo de visualizar un informe referente a nuestro dataset, ejecuta- mos las celdas: En la celda [7] enviamos el informe a un archivo HTML y recibimos la advertencia (warning) de actualizar la utilidad Pillow para evitar advertencias del tipo ValueError. En la [17] tomamos una muestra de tamaño 10% del tamaño del archivo “train.csv”, que es nuestro dataset principal, con sample = large_df.sample(frac=0.1). En la [18] la variable profile recibe el informe generado por la función ProfileReport de la muestra sample, le pone un título con title y pone el parámetro explorative a verdadero. En la [13] se muestra por pantalla de salida y en la siguiente (la que está en ejecución) se redi- rige el profile obtenido a un archivo HTML denominado house_prices.html que podemos vi- sualizar con cualquier navegador.